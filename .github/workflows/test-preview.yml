name: Test Preview Deployment

on:
  workflow_call:
    inputs:
      preview_url:
        description: 'Preview URL to test'
        required: true
        type: string


jobs:
  test-preview:
    name: Run Tests on Preview
    runs-on: blacksmith-8vcpu-ubuntu-2404

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Extract preview URL
        id: extract-url
        run: |
          PREVIEW_URL="${{ inputs.preview_url }}"

          echo "Preview URL: $PREVIEW_URL"
          echo "preview_url=$PREVIEW_URL" >> $GITHUB_OUTPUT

          # Validate URL format
          if [[ "$PREVIEW_URL" =~ ^https?:// ]]; then
            echo "url_valid=true" >> $GITHUB_OUTPUT
          else
            echo "url_valid=false" >> $GITHUB_OUTPUT
          fi

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: ".nvmrc"
          cache: "npm"

      - name: Cache Chrome dependencies
        id: cache-chrome-deps
        uses: actions/cache@v4
        with:
          path: |
            /var/cache/apt/archives/
            /usr/bin/chromium-browser
            /usr/bin/chromedriver
            /usr/bin/xvfb
          key: chrome-deps-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            chrome-deps-${{ runner.os }}-

      - name: Install Chrome dependencies
        if: steps.cache-chrome-deps.outputs.cache-hit != 'true'
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            chromium-browser \
            chromium-chromedriver \
            xvfb

      - name: Install dependencies
        run: npm ci

      - name: Create test environment file
        if: steps.extract-url.outputs.url_valid == 'true'
        run: |
          PREVIEW_URL="${{ steps.extract-url.outputs.preview_url }}"

          cat > tests/.env << EOF
          # The base URL that will be used for the test
          BASE_URL=${PREVIEW_URL}/meet/

          # Room name configuration
          ROOM_NAME_PREFIX=test
          ROOM_NAME_SUFFIX=grid

          # Browser configuration
          HEADLESS=true
          ALLOW_INSECURE_CERTS=true

          # Additional Chrome arguments for stability
          CHROME_ARGS=--disable-web-security --disable-features=VizDisplayCompositor --disable-background-timer-throttling --disable-backgrounding-occluded-windows --disable-renderer-backgrounding --disable-field-trial-config --disable-back-forward-cache --disable-ipc-flooding-protection --enable-features=NetworkService,NetworkServiceLogging --force-color-profile=srgb --metrics-recording-only --no-first-run --enable-automation --password-store=basic --use-mock-keychain --disable-extensions --disable-plugins --disable-default-apps --disable-sync --disable-translate --hide-scrollbars --mute-audio --no-sandbox --disable-dev-shm-usage --disable-setuid-sandbox --disable-gpu --disable-software-rasterizer --disable-background-networking --no-default-browser-check --no-pings --no-zygote --single-process

          # Video capture file (absolute path)
          VIDEO_CAPTURE_FILE=${{ github.workspace }}/tests/resources/fakeVideoStream.y4m

          # Test execution configuration
          MAX_INSTANCES=1

          # Domain resolution
          RESOLVER_RULES=MAP localhost 127.0.0.1
          EOF

          echo "Created tests/.env with BASE_URL: ${PREVIEW_URL}/meet/"

      - name: Run tests
        if: steps.extract-url.outputs.url_valid == 'true'
        id: run-tests
        continue-on-error: true
        run: |
          # Set display for headless Chrome
          export DISPLAY=:99
          # Suppress XKEYBOARD warnings
          export XKB_LOG_LEVEL=critical
          Xvfb :99 -screen 0 1280x1024x24 &

          # Run tests
          npm run test
        env:
          DOTENV_CONFIG_PATH: tests/.env
          XKB_LOG_LEVEL: critical
          QT_X11_NO_MITSHM: 1
          _JAVA_AWT_WM_NONREPARENTING: 1

      - name: Upload test results
        if: always()
        id: upload-test-results
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ github.sha }}
          path: |
            test-results/
            !test-results/allure-results/
          retention-days: 30

      - name: Upload Allure report
        if: always()
        id: upload-allure-report
        uses: actions/upload-artifact@v4
        with:
          name: allure-report-${{ github.sha }}
          path: test-results/allure-report/
          retention-days: 30

      - name: Parse test results for PR comment
        if: always()
        id: parse-results
        run: |
          PREVIEW_URL="${{ steps.extract-url.outputs.preview_url }}"
          URL_VALID="${{ steps.extract-url.outputs.url_valid }}"

          if [ "$URL_VALID" != "true" ]; then
            echo "test_status=failed" >> $GITHUB_OUTPUT
            echo "test_reason=Invalid preview URL" >> $GITHUB_OUTPUT
            echo "test_summary=âŒ **Status:** Failed - Invalid preview URL" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Try to parse JUnit results if available
          if [ -f "test-results/results-0.xml" ]; then
            TESTS=$(grep -o 'tests="[0-9]*"' test-results/results-0.xml | grep -o '[0-9]*' || echo "unknown")
            FAILURES=$(grep -o 'failures="[0-9]*"' test-results/results-0.xml | grep -o '[0-9]*' || echo "0")
            ERRORS=$(grep -o 'errors="[0-9]*"' test-results/results-0.xml | grep -o '[0-9]*' || echo "0")

            # Determine overall test status
            if [ "$FAILURES" -gt 0 ] || [ "$ERRORS" -gt 0 ]; then
              echo "test_status=failed" >> $GITHUB_OUTPUT
              echo "test_summary=âŒ **Status:** Tests failed" >> $GITHUB_OUTPUT
            else
              echo "test_status=passed" >> $GITHUB_OUTPUT
              echo "test_summary=âœ… **Status:** Tests passed" >> $GITHUB_OUTPUT
            fi

            echo "test_results=**Test Results:**" >> $GITHUB_OUTPUT
            echo "test_results<<EOF" >> $GITHUB_OUTPUT
            echo "- Total Tests: $TESTS" >> $GITHUB_OUTPUT
            echo "- Failures: $FAILURES" >> $GITHUB_OUTPUT
            echo "- Errors: $ERRORS" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            # Check if test step failed but no results file exists
            if [ "${{ steps.run-tests.outcome }}" == "failure" ]; then
              echo "test_status=failed" >> $GITHUB_OUTPUT
              echo "test_summary=âŒ **Status:** Tests failed (no detailed results available)" >> $GITHUB_OUTPUT
              echo "test_results=**Note:** Test execution failed - no detailed results available" >> $GITHUB_OUTPUT
            else
              echo "test_status=unknown" >> $GITHUB_OUTPUT
              echo "test_summary=âš ï¸ **Status:** Tests completed (detailed results not available)" >> $GITHUB_OUTPUT
              echo "test_results=**Note:** Detailed test results not available" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Update PR comment with test results
        if: github.event_name == 'pull_request'
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          header: test-results
          message: |
            ## ğŸ§ª Test Results

            ${{ steps.parse-results.outputs.test_summary }}
            **Preview URL:** ${{ steps.extract-url.outputs.url_valid == 'true' && format('[{0}/meet/testmeeting1]({0}/meet/testmeeting1)', steps.extract-url.outputs.preview_url) || 'Preview URL not available' }}

            ${{ steps.parse-results.outputs.test_reason && format('**Reason:** {0}', steps.parse-results.outputs.test_reason) || '' }}

            ${{ steps.parse-results.outputs.test_results }}

            **Artifacts:**
            - [ğŸ“Š Test Results](${{ steps.upload-test-results.outputs.artifact-url }})
            - [ğŸ“ˆ Allure Report](${{ steps.upload-allure-report.outputs.artifact-url }})

            ---
            *This comment will be automatically updated when new commits are pushed.*

      - name: Fail job if tests failed
        if: always()
        run: |
          # Only fail if the test step actually failed
          if [ "${{ steps.run-tests.outcome }}" == "failure" ]; then
            # Check if we have test result files to determine the type of failure
            if [ -f "test-results/results-0.xml" ]; then
              FAILURES=$(grep -o 'failures="[0-9]*"' test-results/results-0.xml | grep -o '[0-9]*' || echo "0")
              ERRORS=$(grep -o 'errors="[0-9]*"' test-results/results-0.xml | grep -o '[0-9]*' || echo "0")

              if [ "$FAILURES" -gt 0 ] || [ "$ERRORS" -gt 0 ]; then
                echo "âŒ Tests failed with $FAILURES failures and $ERRORS errors"
                echo "This is a test failure - failing the job"
                exit 1
              else
                echo "âŒ Test execution failed but no test failures detected"
                echo "This might be a setup issue - failing the job"
                exit 1
              fi
            else
              echo "âŒ Test execution failed and no result files found"
              echo "This is likely a setup failure - failing the job"
              exit 1
            fi
          else
            echo "âœ… Test step completed successfully"
          fi

